{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">PART III</h1>\n",
    "<h1 align=\"center\">Training and Development of Different LSTM Models</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, let's download the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import gensim, logging\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also let's write a style for alignment in the middle of all graphs, images, etc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".output_png {\n",
    "    display: table-cell;\n",
    "    text-align: center;\n",
    "    vertical-align: middle;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Preparation of input data</h2> \n",
    "\n",
    "Firstly, we will load the sample data we processed in the first part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('documents.pql', 'rb') as f:\n",
    "     docs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of documents:\", len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will load our word2vec model. \n",
    "\n",
    "This may take some time, as the model contains 400 000 words, so we will get a 400 000 x 50 embedding matrix that contains all the values of the word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.KeyedVectors.load_word2vec_format('model/glove.6B.50d.w2vformat.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get a list of all the words from our dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(model.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to make sure everything is loaded correctly, we can look at the dimensions of the dictionary list and the embedding matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total words:\", len(words), \"\\n\\nWord-Vectors shape:\", model.vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of our sample for training will be `(H, N, 50)`, where:\n",
    "* `H` - number of samples;\n",
    "* `N` - number of words in each sentence;\n",
    "* `50` - dimension of each word.\n",
    "\n",
    "Let's analyze how many words are usually found in the reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_word, min_word = 0, 100\n",
    "word50, word100, word200, word300, word400  = 0, 0, 0, 0, 0\n",
    "reviews_len = []\n",
    "\n",
    "for review, state in docs:\n",
    "    reviews_len.append(len(review))\n",
    "\n",
    "    if len(review) > max_word: max_word = len(review)\n",
    "    if len(review) < min_word: min_word = len(review)\n",
    "\n",
    "    if len(review) > 50: word50 += 1\n",
    "    if len(review) > 100: word100 += 1\n",
    "    if len(review) > 200: word200 += 1\n",
    "    if len(review) > 300: word300 += 1\n",
    "    if len(review) > 400: word400 += 1\n",
    "    \n",
    "print(\"Average number of words in the review:\", int(sum(reviews_len)/len(reviews_len)))\n",
    "print(\"\\nMaximum review length:\", max_word, \"\\nMinimum review length:\", min_word)\n",
    "print(\"\\nReview with more than 50 words:\", word50, \n",
    "      \"\\nReview with more than 100 words:\", word100,\n",
    "      \"\\nReview with more than 200 words:\", word200, \n",
    "      \"\\nReview with more than 300 words:\", word300,\n",
    "      \"\\nReview with more than 400 words:\", word400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the frequency of words in the review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(reviews_len, bins=150)\n",
    "plt.axis([0, 400, 0, 12000])\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Sequence Length');\n",
    "plt.title(\"Average number of words\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next function `fix_review_len()` will do a useful job, this function is designed to fix the size of reviews to a fixed size to feed them into a neural network with reviews of a certain length. Reviews whose length is less than fixed will be extended by zeros. This process does not affect the algorithm and reviews longer than the specified length will be truncated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_review_len(review, length):\n",
    "    if len(review) > length:\n",
    "        review = review[:length]\n",
    "    elif len(review) < length:\n",
    "        for i in range(length - len(review)):\n",
    "            zeros = [0] * 50\n",
    "            review.append(zeros)\n",
    "    return review\n",
    "\n",
    "example = [3, 1, 2, 4, 5]\n",
    "example = fix_review_len(example, 7)\n",
    "print(example) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's slightly improve our `sent_embed()` function from previous part and then update our dataset with word vectors.\n",
    "\n",
    "Based on the histogram data, as well as the average number of words in the files, we can say with confidence that most reviews will have less than 100 words, which is the maximum value of the length of the sequence that we set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_embed(words, docs):\n",
    "    x_sent_embed, y_sent_embed = [], [] \n",
    "    \n",
    "    max_seq_len = 100\n",
    "    \n",
    "    # recover the embedding of each sentence with the average of the vector that composes it\n",
    "    # sent - sentence, state - state of the sentence (pos/neg)\n",
    "    for sent, state in docs:\n",
    "        # average embedding of all words in a sentence\n",
    "        sent_embed = []\n",
    "        for word in sent:\n",
    "            try:\n",
    "                # if word is present in the dictionary - add its vector representation\n",
    "                sent_embed.append(model[word])\n",
    "            except KeyError:\n",
    "                # if word is not in the dictionary - add a zero vector\n",
    "                sent_embed.append([0] * 50)\n",
    "        \n",
    "        # add a sentence vector to the list\n",
    "        sent_embed = fix_review_len(sent_embed, max_seq_len)\n",
    "        x_sent_embed.append(sent_embed)\n",
    "        \n",
    "        # add a label to y_sent_embed\n",
    "        if state == 'pos': y_sent_embed.append(1)\n",
    "        elif state == 'neg': y_sent_embed.append(0)\n",
    "            \n",
    "    return x_sent_embed, y_sent_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = sent_embed(words, docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(x)\n",
    "print(\"Shape of X:\", x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Split Corpus</h3>\n",
    "\n",
    "Now, for further work, we will divide our corpus for training, testing and development sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.15, random_state=42)\n",
    "\n",
    "# train dev\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Length of x_train:', len(x_train), '| Length of y_train:', len(y_train))\n",
    "print('Length of x_test:  ', len(x_test), '| Length of y_test: ', len(y_test))\n",
    "print('Length of x_val:   ', len(x_val), '| Length of y_val:  ', len(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of x_train set:\", x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">LSTM Model (Batch Size)</h2>\n",
    "\n",
    "Keras's benefit is that it is built on top of symbolic mathematical libraries such as TensorFlow and Theano for fast and efficient computation. This is needed with large neural networks.\n",
    "\n",
    "A downside of using these efficient libraries is that you must define the scope of your data upfront and for all time. Specifically, the batch size.\n",
    "\n",
    "The batch size limits the number of samples to be shown to the network before a weight update can be performed. This same limitation is then imposed when making predictions with the fit model.\n",
    "\n",
    "Specifically, the batch size used when fitting your model controls how many predictions you must make at a time.\n",
    "\n",
    "This is often not a problem when you want to make the same number predictions at a time as the batch size used during training.\n",
    "\n",
    "This does become a problem when you wish to make fewer predictions than the batch size. For example, you may get the best results with a large batch size but are required to make predictions for one observation at a time on something like a time series or sequence problem.\n",
    "\n",
    "This is why it may be desirable to have a different batch size when fitting the network to training data than when making predictions on test data or new input data.\n",
    "\n",
    "Batch gradient descent is a variation of the gradient descent algorithm that calculates the error for each example in the training dataset, but only updates the model after all training examples have been evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Setting Up and Creating the Network</h3>\n",
    "\n",
    "The network has one input, a hidden layer with 100 units, and an output layer with 1 unit.\n",
    "\n",
    "A mean squared error optimization function is used for this problem with the efficient ADAM optimization algorithm.\n",
    "\n",
    "So now let's configure and create the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# configure network\n",
    "n_neurons = 100\n",
    "n_epoch = 20\n",
    "n_batch = 16 # actually we need 1, but my laptop will do it ~5 hours\n",
    "\n",
    "# design network\n",
    "model_batch = Sequential()\n",
    "model_batch.add(LSTM(n_neurons, input_shape=(x.shape[1], x.shape[2])))\n",
    "model_batch.add(Dense(1, activation='sigmoid'))\n",
    "model_batch.compile(loss='mean_squared_error', optimizer='adam', metrics=['binary_accuracy'])\n",
    "\n",
    "print(model_batch.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_batch_hist = model_batch.fit(x_train, np.asarray(y_train), batch_size=n_batch, epochs=n_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_batch = model_batch.evaluate(x_val, np.asarray(y_val), batch_size=n_batch)\n",
    "print(\"\\nModel Accuracy:\",str(round(score_batch[1] * 100, 2)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "plt.plot(model_batch_hist.history['loss'])\n",
    "\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train'], loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are satisfactory, let's save this model and try a different approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model_batch.save('models/LSTM-Batch-Model.h5')  # save model\n",
    "del model_batch  # delete existing model\n",
    "\n",
    "# returns a compiled model, identical to the previous one\n",
    "model_batch = load_model('models/LSTM-Batch-Model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if everything is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_batch.evaluate(x_val, np.asarray(y_val), batch_size=n_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">LSTM Model (Mini-Batch Gradient Descent)</h2>\n",
    "\n",
    "Mini-batch gradient descent is a variation of the gradient descent algorithm that splits the training dataset into small batches that are used to calculate model error and update model coefficients.\n",
    "\n",
    "Mini batch algorithm is the most favorable and widely used algorithm that makes precise and faster results using a batch of `m` training examples. In mini batch algorithm rather than using  the complete data set, in every iteration we use a set of `m` training examples called batch to compute the gradient of the cost function. Common mini-batch sizes range between 32 and 256."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Setting Up and Creating the Network</h3>\n",
    "\n",
    "We will use an LSTM network fit for 100 epochs and with batch size = 128.\n",
    "\n",
    "A mean squared error optimization function is used for this problem with the efficient ADAM optimization algorithm.\n",
    "\n",
    "`recurrent_dropout`: float between 0 and 1. Fraction of the input units to drop for recurrent connections. \n",
    "\n",
    "`dropout`: float between 0 and 1. Fraction of the input units to drop for input gates.\n",
    "\n",
    "\n",
    "So now let's configure and create the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Dropout\n",
    "\n",
    "# configure network\n",
    "n_neurons = 100\n",
    "n_epoch = 100\n",
    "n_batch = 128\n",
    "\n",
    "# design network\n",
    "model_mini = Sequential()\n",
    "model_mini.add(LSTM(n_neurons, input_shape=(x.shape[1], x.shape[2]), dropout=0.2, recurrent_dropout=0.2))\n",
    "model_mini.add(Dense(1, activation='sigmoid'))\n",
    "model_mini.compile(loss='mean_squared_error', optimizer='adam', metrics=['binary_accuracy'])\n",
    "\n",
    "print(model_mini.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mini_hist = model_mini.fit(x_train, np.asarray(y_train), batch_size=n_batch, epochs=n_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mini_score = model_mini.evaluate(x_val, np.asarray(y_val), batch_size=n_batch)\n",
    "print(\"\\nModel Accuracy:\",str(round(model_mini_score[1] * 100, 2)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model_mini_hist.history['loss'])\n",
    "\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train'], loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a good value for the model, we can use it. Let's save our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mini.save('models/LSTM-Mini-Batch-Model.h5')  # save model\n",
    "del model_mini  # delete existing model\n",
    "\n",
    "# returns a compiled model, identical to the previous one\n",
    "model_mini = load_model('models/LSTM-Mini-Batch-Model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if everything is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mini.evaluate(x_val, np.asarray(y_val), batch_size=n_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Stacked LSTM Layers</h2>\n",
    "\n",
    "Now we will try the model with a 3 stacked LSTM layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure network\n",
    "n_neurons = 100\n",
    "n_epoch = 20\n",
    "n_batch = 128\n",
    "\n",
    "# design network\n",
    "model3L = Sequential()\n",
    "\n",
    "model3L.add(LSTM(n_neurons, return_sequences=True, input_shape=(x.shape[1], x.shape[2])))\n",
    "model3L.add(LSTM(32, return_sequences=True))  # returns a sequence of vectors of dimension 32\n",
    "model3L.add(LSTM(32))  # return a single vector of dimension 32\n",
    "\n",
    "model3L.add(Dense(1, activation='sigmoid'))\n",
    "model3L.compile(loss='mean_squared_error', optimizer='adam', metrics=['binary_accuracy'])\n",
    "\n",
    "print(model3L.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3L_hist = model3L.fit(x_train, np.asarray(y_train), batch_size=n_batch, epochs=n_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3L_score = model3L.evaluate(x_val, np.asarray(y_val), batch_size=n_batch)\n",
    "print(\"\\nModel Accuracy:\",str(round(model3L_score[1] * 100, 2)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model3L_hist.history['loss'])\n",
    "\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train'], loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3L.save('models/LSTM-3L-Model.h5')  # save model\n",
    "del model3L  # delete existing model\n",
    "\n",
    "# returns a compiled model, identical to the previous one\n",
    "model3L = load_model('models/LSTM-3L-Model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if everything is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3L.evaluate(x_val, np.asarray(y_val), batch_size=n_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">CNN + LSTM</h2>\n",
    "\n",
    "This approach is based entirely on [this work](https://www.aclweb.org/anthology/P16-2037.pdf). As indicated in the work:\n",
    "\n",
    "<cite>\n",
    "    \n",
    "    Dimensional sentiment analysis aims to recognize continuous numerical values in multiple dimensions such as the valencearousal (VA) space. \n",
    "    \n",
    "    Compared to the categorical approach that focuses on sentiment classification such as binary classification (i.e., positive and negative), the dimensional approach can provide more fine-grained sentiment analysis. This study proposes a regional CNN-LSTM model consisting of two parts: regional CNN and LSTM to predict the VA ratings of texts. \n",
    "   \n",
    "    Unlike a conventional CNN which considers a whole text as input, the proposed regional CNN uses an individual sentence as a region, dividing an input text into several regions such that the useful affective information in each region can be extracted and weighted according to their contribution to the VA prediction. Such regional information is sequentially integrated across regions using LSTM for VA prediction. \n",
    "\n",
    "</cite>\n",
    "\n",
    "In short, word vectors of vocabulary words are trained from a large corpus using the word2vec toolkit. For each given text, the  regional CNN model uses the sentence as a region to divide the text into R-domains, ie r1, ..., ri rj, rk, ..., rR. In each region, useful affective functions can be removed when word vectors pass sequentially through the convolutional layer and the  maxpooling  layer. Then such local (regional) features are sequentially integrated between regions using LSTM to construct a text vector for VA prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "\n",
    "# configure network\n",
    "n_epoch = 20\n",
    "n_batch = 128\n",
    "\n",
    "# design network\n",
    "model_CNN_LSTM = Sequential()\n",
    "\n",
    "model_CNN_LSTM.add(Conv1D(filters=100, kernel_size=3, input_shape=(x.shape[1], x.shape[2]), padding='same', activation='relu'))\n",
    "model_CNN_LSTM.add(MaxPooling1D(pool_size=2))\n",
    "model_CNN_LSTM.add(LSTM(100))\n",
    "model_CNN_LSTM.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_CNN_LSTM.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model_CNN_LSTM.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_CNN_LSTM_hist = model_CNN_LSTM.fit(x_train, np.asarray(y_train), epochs=n_epoch, batch_size=n_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_CNN_LSTM_score = model_CNN_LSTM.evaluate(x_val, np.asarray(y_val), batch_size=n_batch)\n",
    "print(\"\\nModel Accuracy:\",str(round(model_CNN_LSTM_score[1] * 100, 2)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model_CNN_LSTM_hist.history['loss'])\n",
    "\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train'], loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_CNN_LSTM.save('models/LSTM-CNN-Model.h5')  # save model\n",
    "del model_CNN_LSTM  # delete existing model\n",
    "\n",
    "# returns a compiled model, identical to the previous one\n",
    "model_CNN_LSTM = load_model('models/LSTM-CNN-Model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Bidirectional LSTM</h2>\n",
    "\n",
    "Bidirectional LSTMs are an extension of traditional LSTMs that can improve model performance on sequence classification problems.\n",
    "\n",
    "In problems where all timesteps of the input sequence are available, Bidirectional LSTMs train two instead of one LSTMs on the input sequence. The first on the input sequence as-is and the second on a reversed copy of the input sequence. This can provide additional context to the network and result in faster and even fuller learning on the problem.\n",
    "\n",
    "The idea of Bidirectional Recurrent Neural Networks (RNNs) is straightforward.\n",
    "\n",
    "It involves duplicating the first recurrent layer in the network so that there are now two layers side-by-side, then providing the input sequence as-is as input to the first layer and providing a reversed copy of the input sequence to the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional\n",
    "from keras.datasets import imdb\n",
    "\n",
    "# configure network\n",
    "max_seq_len = 100\n",
    "n_epoch = 20\n",
    "n_batch = 128\n",
    "\n",
    "# design network\n",
    "model_Bi_LSTM = Sequential()\n",
    "model_Bi_LSTM.add(Bidirectional(LSTM(64), input_shape=(x.shape[1], x.shape[2])))\n",
    "model_Bi_LSTM.add(Dropout(0.5))\n",
    "model_Bi_LSTM.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model_Bi_LSTM.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "print(modelBiLstm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Bi_LSTM_hist = model_Bi_LSTM.fit(x_train, np.asarray(y_train), epochs=n_epoch, batch_size=n_batch, validation_data=[x_val, np.asarray(y_val)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Bi_LSTM_score = model_Bi_LSTM.evaluate(x_val, np.asarray(y_val), batch_size=n_batch)\n",
    "print(\"\\nModel Accuracy:\",str(round(model_CNN_LSTM_score[1] * 100, 2)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model_Bi_LSTM_hist.history['loss'])\n",
    "plt.plot(model_Bi_LSTM_hist.history['val_loss'])\n",
    "\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model_Bi_LSTM_hist.history['acc'])\n",
    "plt.plot(model_Bi_LSTM_hist.history['val_acc'])\n",
    "\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Bi_LSTM.save('models/LSTM-Bi-Model.h5')  # save model\n",
    "del model_Bi_LSTM  # delete existing model\n",
    "\n",
    "# returns a compiled model, identical to the previous one\n",
    "model_Bi_LSTM = load_model('models/LSTM-Bi-Model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
