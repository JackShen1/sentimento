{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">PART II</h1>\n",
    "<h1 align=\"center\">Sentiment Analysis Classifications - Review and Comparison</h1>\n",
    "\n",
    "First, we needed to create vector words. For simplicity, we used a pre-trained model.\n",
    "\n",
    "Google was able to teach the Word2Vec model on a massive Google News dataset that contained over 100 billion different words! Google has created [3 million vector words](https://code.google.com/archive/p/word2vec/#Pre-trained_word_and_phrase_vectors) from this model, each with a dimension of 300.\n",
    "\n",
    "Ideally, we would use these vectors, but because the vector-word matrix is quite large (3.6 GB), we used a much more manageable matrix, which was trained using [GloVe](https://nlp.stanford.edu/projects/glove/), with a similar model of vector word generation. This matrix contains 400,000 vector words, each with a dimension of 50. You can also download model [here](https://www.kaggle.com/anindya2906/glove6b?select=glove.6B.50d.txt).\n",
    "\n",
    "#### How word2vec works:\n",
    "\n",
    "The idea behind word2vec is that:\n",
    "\n",
    "    Take a 3 layer neural network. (1 input layer + 1 hidden layer + 1 output layer)\n",
    "    Feed it a word and train it to predict its neighbouring word.\n",
    "    Remove the last (output layer) and keep the input and hidden layer.\n",
    "    Now, input a word from within the vocabulary. The output given at the hidden layer is the ‘word embedding’ of the input word.\n",
    "    \n",
    "Two popular examples of methods of learning word embeddings from text include:\n",
    "\n",
    "    Word2Vec\n",
    "    GloVe\n",
    "\n",
    "To get started, let's download the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import gensim, logging\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also let's write a style for alignment in the middle of all graphs, images, etc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".output_png {\n",
       "    display: table-cell;\n",
       "    text-align: center;\n",
       "    vertical-align: middle;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".output_png {\n",
    "    display: table-cell;\n",
    "    text-align: center;\n",
    "    vertical-align: middle;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will load the sample data we processed in the previous part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('documents.pql', 'rb') as f:\n",
    "     docs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 38544\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of documents:\", len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will load our glove model in word2vec format. Because the GloVe dump from Stanford's site is slightly different from the word2vec format. You can convert a GloVe file to word2vec format using the following command in your console:\n",
    "\n",
    "`python -m gensim.scripts.glove2word2vec --input  model/glove.6B.50d.txt --output model/glove.6B.50d.w2vformat.txt`\n",
    "\n",
    "After that you can delete original GloVe model.\n",
    "\n",
    "Next operation may take some time, as the model contains 400 000 words, so we will get a 400 000 x 50 embedding matrix that contains all the values of the word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.KeyedVectors.load_word2vec_format('model/glove.6B.50d.w2vformat.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get a list of all the words from our dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(model.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to make sure everything is loaded correctly, we can look at the dimensions of the dictionary list and the embedding matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', ',', '.', 'of', 'to', 'and', 'in', 'a', '\"', \"'s\", 'for', '-', 'that', 'on', 'is', 'was', 'said', 'with', 'he', 'as', 'it', 'by', 'at', '(', ')', 'from', 'his', \"''\", '``', 'an', 'be', 'has', 'are', 'have', 'but', 'were', 'not', 'this', 'who', 'they', 'had', 'i', 'which', 'will', 'their', ':', 'or', 'its', 'one', 'after'] \n",
      "\n",
      "Total words: 400000 \n",
      "\n",
      "Word-Vectors shape: (400000, 50)\n"
     ]
    }
   ],
   "source": [
    "print(words[:50], \"\\n\\nTotal words:\", len(words), \"\\n\\nWord-Vectors shape:\", model.vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also find a word like \"football\" in our word list and then access the corresponding vector through the embedding matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.8209    0.70094  -1.1403    0.34363  -0.42266  -0.92479  -1.3942\n",
      "  0.28512  -0.78416  -0.52579   0.89627   0.35899  -0.80087  -0.34636\n",
      "  1.0854   -0.087046  0.63411   1.1429   -1.6264    0.41326  -1.1283\n",
      " -0.16645   0.17424   0.99585  -0.81838  -1.7724    0.078281  0.13382\n",
      " -0.59779  -0.45068   2.5474    1.0693   -0.27017  -0.75646   0.24757\n",
      "  1.0261    0.11329   0.17668  -0.23257  -1.1561   -0.10665  -0.25377\n",
      " -0.65102   0.32393  -0.58262   0.88137  -0.13465   0.96903  -0.076259\n",
      " -0.59909 ]\n"
     ]
    }
   ],
   "source": [
    "print(model['football'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Word Average Embedding Model</h2>\n",
    "\n",
    "Well, let's start analyzing our vectors. Our first approach will be the **word average embedding model**. \n",
    "\n",
    "The essence of this naive approach is to take the average of all word vectors from a sentence to get one 50-dimensional vector that represents the tone of the whole sentence that we feed the model and try to get some quick result.\n",
    "\n",
    "We didn't have to put a try/except, but even though I cleaned up our sample, there were a couple of words left after the processing that needed to be searched for and removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_embed(words, docs):\n",
    "    x_sent_embed, y_sent_embed = [], []\n",
    "    count_words, count_non_words = 0, 0  \n",
    "    \n",
    "    # recover the embedding of each sentence with the average of the vector that composes it\n",
    "    # sent - sentence, state - state of the sentence (pos/neg)\n",
    "    for sent, state in docs:\n",
    "        # average embedding of all words in a sentence\n",
    "        sent_embed = []\n",
    "        for word in sent:\n",
    "            try:\n",
    "                # if word is present in the dictionary - add its vector representation\n",
    "                count_words += 1\n",
    "                sent_embed.append(model[word])\n",
    "            except KeyError:\n",
    "                # if word is not in the dictionary - add a zero vector\n",
    "                count_non_words += 1\n",
    "                sent_embed.append([0] * 50)\n",
    "        \n",
    "        # add a sentence vector to the list\n",
    "        x_sent_embed.append(np.mean(sent_embed, axis=0).tolist())\n",
    "        \n",
    "        # add a label to y_sent_embed\n",
    "        if state == 'pos': y_sent_embed.append(1)\n",
    "        elif state == 'neg': y_sent_embed.append(0)\n",
    "            \n",
    "    print(count_non_words, \"out of\", count_words, \"words were not found in the vocabulary.\")\n",
    "    \n",
    "    return x_sent_embed, y_sent_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30709 out of 1802696 words were not found in the vocabulary.\n"
     ]
    }
   ],
   "source": [
    "x, y = sent_embed(words, docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
