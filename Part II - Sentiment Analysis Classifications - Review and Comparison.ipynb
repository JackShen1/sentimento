{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">PART II</h1>\n",
    "<h1 align=\"center\">Sentiment Analysis Classifications - Review and Comparison</h1>\n",
    "\n",
    "First, we needed to create vector words. For simplicity, we used a pre-trained model.\n",
    "\n",
    "Google was able to teach the Word2Vec model on a massive Google News dataset that contained over 100 billion different words! Google has created [3 million vector words](https://code.google.com/archive/p/word2vec/#Pre-trained_word_and_phrase_vectors) from this model, each with a dimension of 300.\n",
    "\n",
    "Ideally, we would use these vectors, but because the vector-word matrix is quite large (3.6 GB), we used a much more manageable matrix, which was trained using [GloVe](https://nlp.stanford.edu/projects/glove/), with a similar model of vector word generation. This matrix contains 400,000 vector words, each with a dimension of 50. You can also download model [here](https://www.kaggle.com/anindya2906/glove6b?select=glove.6B.50d.txt).\n",
    "\n",
    "#### How word2vec works:\n",
    "\n",
    "The idea behind word2vec is that:\n",
    "\n",
    "    Take a 3 layer neural network. (1 input layer + 1 hidden layer + 1 output layer)\n",
    "    Feed it a word and train it to predict its neighbouring word.\n",
    "    Remove the last (output layer) and keep the input and hidden layer.\n",
    "    Now, input a word from within the vocabulary. The output given at the hidden layer is the ‘word embedding’ of the input word.\n",
    "    \n",
    "Two popular examples of methods of learning word embeddings from text include:\n",
    "\n",
    "    Word2Vec\n",
    "    GloVe\n",
    "\n",
    "To get started, let's download the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import gensim, logging\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also let's write a style for alignment in the middle of all graphs, images, etc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".output_png {\n",
       "    display: table-cell;\n",
       "    text-align: center;\n",
       "    vertical-align: middle;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".output_png {\n",
    "    display: table-cell;\n",
    "    text-align: center;\n",
    "    vertical-align: middle;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will load the sample data we processed in the previous part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('documents.pql', 'rb') as f:\n",
    "     docs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 38544\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of documents:\", len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will load our glove model in word2vec format. Because the GloVe dump from Stanford's site is slightly different from the word2vec format. You can convert a GloVe file to word2vec format using the following command in your console:\n",
    "\n",
    "`python -m gensim.scripts.glove2word2vec --input  model/glove.6B.50d.txt --output model/glove.6B.50d.w2vformat.txt`\n",
    "\n",
    "After that you can delete original GloVe model.\n",
    "\n",
    "Next operation may take some time, as the model contains 400 000 words, so we will get a 400 000 x 50 embedding matrix that contains all the values of the word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.KeyedVectors.load_word2vec_format('model/glove.6B.50d.w2vformat.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get a list of all the words from our dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(model.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to make sure everything is loaded correctly, we can look at the dimensions of the dictionary list and the embedding matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', ',', '.', 'of', 'to', 'and', 'in', 'a', '\"', \"'s\", 'for', '-', 'that', 'on', 'is', 'was', 'said', 'with', 'he', 'as', 'it', 'by', 'at', '(', ')', 'from', 'his', \"''\", '``', 'an', 'be', 'has', 'are', 'have', 'but', 'were', 'not', 'this', 'who', 'they', 'had', 'i', 'which', 'will', 'their', ':', 'or', 'its', 'one', 'after'] \n",
      "\n",
      "Total words: 400000 \n",
      "\n",
      "Word-Vectors shape: (400000, 50)\n"
     ]
    }
   ],
   "source": [
    "print(words[:50], \"\\n\\nTotal words:\", len(words), \"\\n\\nWord-Vectors shape:\", model.vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also find a word like \"football\" in our word list and then access the corresponding vector through the embedding matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.8209    0.70094  -1.1403    0.34363  -0.42266  -0.92479  -1.3942\n",
      "  0.28512  -0.78416  -0.52579   0.89627   0.35899  -0.80087  -0.34636\n",
      "  1.0854   -0.087046  0.63411   1.1429   -1.6264    0.41326  -1.1283\n",
      " -0.16645   0.17424   0.99585  -0.81838  -1.7724    0.078281  0.13382\n",
      " -0.59779  -0.45068   2.5474    1.0693   -0.27017  -0.75646   0.24757\n",
      "  1.0261    0.11329   0.17668  -0.23257  -1.1561   -0.10665  -0.25377\n",
      " -0.65102   0.32393  -0.58262   0.88137  -0.13465   0.96903  -0.076259\n",
      " -0.59909 ]\n"
     ]
    }
   ],
   "source": [
    "print(model['football'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Word Average Embedding Model</h2>\n",
    "\n",
    "Well, let's start analyzing our vectors. Our first approach will be the **word average embedding model**. \n",
    "\n",
    "The essence of this naive approach is to take the average of all word vectors from a sentence to get one 50-dimensional vector that represents the tone of the whole sentence that we feed the model and try to get some quick result.\n",
    "\n",
    "We didn't have to put a try/except, but even though I cleaned up our sample, there were a couple of words left after the processing that needed to be searched for and removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_embed(words, docs):\n",
    "    x_sent_embed, y_sent_embed = [], []\n",
    "    count_words, count_non_words = 0, 0  \n",
    "    \n",
    "    # recover the embedding of each sentence with the average of the vector that composes it\n",
    "    # sent - sentence, state - state of the sentence (pos/neg)\n",
    "    for sent, state in docs:\n",
    "        # average embedding of all words in a sentence\n",
    "        sent_embed = []\n",
    "        for word in sent:\n",
    "            try:\n",
    "                # if word is present in the dictionary - add its vector representation\n",
    "                count_words += 1\n",
    "                sent_embed.append(model[word])\n",
    "            except KeyError:\n",
    "                # if word is not in the dictionary - add a zero vector\n",
    "                count_non_words += 1\n",
    "                sent_embed.append([0] * 50)\n",
    "        \n",
    "        # add a sentence vector to the list\n",
    "        x_sent_embed.append(np.mean(sent_embed, axis=0).tolist())\n",
    "        \n",
    "        # add a label to y_sent_embed\n",
    "        if state == 'pos': y_sent_embed.append(1)\n",
    "        elif state == 'neg': y_sent_embed.append(0)\n",
    "            \n",
    "    print(count_non_words, \"out of\", count_words, \"words were not found in the vocabulary.\")\n",
    "    \n",
    "    return x_sent_embed, y_sent_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30709 out of 1802696 words were not found in the vocabulary.\n"
     ]
    }
   ],
   "source": [
    "x, y = sent_embed(words, docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Cosine Similarity</h2>\n",
    "\n",
    "To measure the similarity of 2 words, we need a way to measure the degree of similarity between 2 embedding vectors for these 2 words. Given 2 vectors $u$ and $v$, cosine similarity is determined as follows:\n",
    "\n",
    "$$\\text{cosine_similarity(u, v)} = \\frac {u . v} {||u||_2 ||v||_2} = cos(\\theta)$$\n",
    "\n",
    "where: \n",
    "\n",
    "* $u.v$ - dot product (or inner product) of two vectors;\n",
    "\n",
    "* $||u||_2$ - norm (or length) of the vector $u$;\n",
    "    \n",
    "    * **Note**: norm of $u$ is defined as $ ||u||_2 = \\sqrt{\\sum_{i=1}^{n} u_i^2}$)\n",
    "\n",
    "* $\\theta$ is the angle between $u$ and $v$. \n",
    "\n",
    "This similarity depends on the angle between $u$ and $v$. If $u$ and $v$ are very similar, their cosine similarity will be close to 1; if they are dissimilar, the cosine similarity will take a smaller value. \n",
    "\n",
    "**`cosine_similarity()`** is a method that used to estimate the similarity between word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(u, v):\n",
    "    \"\"\"\n",
    "    Cosine similarity reflects the degree of similariy between u and v\n",
    "        \n",
    "    Arguments:\n",
    "        u -- a word vector of shape (n,)          \n",
    "        v -- a word vector of shape (n,)\n",
    "\n",
    "    Returns:\n",
    "        cosine_similarity -- the cosine similarity between u and v defined by the formula above.\n",
    "    \"\"\"\n",
    "    \n",
    "    distance = 0.0\n",
    "    \n",
    "    # compute the dot product between u and v\n",
    "    dot = np.dot(u,v)\n",
    "    \n",
    "    # compute the L2 norm of u\n",
    "    norm_u = np.sqrt(sum(u**2))\n",
    "    \n",
    "    # Compute the L2 norm of v\n",
    "    norm_v = np.sqrt(sum(v**2))\n",
    "    \n",
    "    # Compute the cosine similarity defined by formula above\n",
    "    cosine_similarity = dot/(norm_u*norm_v)\n",
    "    \n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the cosine similarity on 2 negative sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence #5:  (['eager', 'read', 'book', 'reading', 'left', 'completely', 'flat', 'rest', 'iceberg', 'hardly', 'smith', 'barely', 'give', 'could', 'reading', 'sport', 'illustrated', 'minnesota', 'sport', 'page', 'admits', 'football', 'majority', 'book', 'leisurely', 'describes', 'basic', 'part', 'football', 'career', 'book', 'touted', 'full', 'complete', 'story', 'learned', 'watching', 'stimpy', 'draft', 'type', 'meaty', 'story', 'want', 'read', 'telling', 'inside', 'detail', 'smith', 'deal', 'people', 'wanted', 'friend', 'girlfriend', 'star', 'football', 'player', 'friend', 'could', 'trust', 'need', 'trust', 'thought', 'enter', 'mind', 'look', 'hire', 'agent', 'goofing', 'missing', 'class', 'summer', 'covered', 'story', 'time', 'ohio', 'state', 'begin', 'using', 'football', 'smart', 'enough', 'study', 'break', 'coaching', 'process', 'mentally', 'challenging', 'happened', 'plan', 'medical', 'school', 'know', 'story', 'football', 'part', 'iceberg', 'already', 'documented', 'also', 'felt', 'smith', 'wrote', 'book', 'defending', 'america', 'attacking', 'feel', 'need', 'explain', 'paid', 'much', 'highly', 'paid', 'athlete', 'story', 'sorry', 'dime', 'dozen', 'compare', 'sitting', 'money', 'scenario', 'average', 'truth', 'many', 'people', 'take', 'worker', 'value', 'thing', 'money', 'happy', 'leaving', 'grammatical', 'error', 'reviewer', 'felt', 'like', 'reading', 'average', 'high', 'school', 'creative', 'writing', 'project', 'went', 'next', 'half', 'quote', 'wellspring', 'deep', 'philosophy', 'music', 'smith', 'note', 'percent', 'american', 'read', '10th', 'grade', 'level', 'well', 'book', 'certainly', 'reading', 'ability', 'many', 'people', 'lesson', 'book', 'smith', 'opportunity', 'swallow', 'pride', 'take', 'coaching', 'come', 'better', 'solution', 'book', 'another', 'time', 'hired', 'editor', 'writing', 'consultant', 'could', 'give', 'guidance', 'build', 'character', 'help', 'feel', 'story', 'instead', 'smith', 'felt', 'necessary', 'alone', 'emotion', 'sorry', 'compared', 'autobiography', 'emotional', 'book', 'sure', 'smith', 'thing', 'mike', 'gutter', 'thoughtful', 'honest', 'trustworthy', 'dependable', 'passionate', 'felt', 'best', 'story', 'would', 'demonstrate', 'characteristic', 'left', 'book', 'instead', 'still', 'looking', 'rest', 'iceberg'], 'neg') \n",
      "\n",
      "Sentence #7:  (['value', 'amazon', 'product', 'review', 'purchased', 'many', 'item', 'based', 'customer', 'feedback', 'purchased', 'phone', 'home', 'depot', 'returning', 'today', 'large', 'comfort', 'reception', 'letter', 'small', 'need', 'reading', 'glass', 'could'], 'neg')\n",
      "\n",
      "Sentence Embedding #5:  [0.01131779607385397, 0.23367555439472198, -0.056668754667043686, -0.24764131009578705, 0.430510550737381, 0.07081332057714462, -0.5177653431892395, -0.03903905302286148, -0.13728252053260803, 0.07700246572494507, -0.16849233210086823, 0.2067411094903946, -0.2801123857498169, -0.0610094778239727, 0.4939613342285156, -0.03934195637702942, 0.09831012785434723, 0.0422029048204422, -0.13356490433216095, -0.3223132789134979, -0.019953656941652298, 0.3334996700286865, 0.1518217921257019, 0.1095254123210907, 0.3594696521759033, -1.2954380512237549, -0.4831620156764984, -0.019770419225096703, 0.25212976336479187, -0.24769696593284607, 2.5406434535980225, 0.12557470798492432, -0.07102328538894653, -0.3904299736022949, 0.004364060703665018, 0.05013019219040871, -0.018428627401590347, 0.1928984671831131, 0.04191068187355995, -0.34819087386131287, -0.03821679949760437, -0.005490944255143404, -0.026860175654292107, 0.24909710884094238, -0.06916779279708862, 0.10717158019542694, -0.020162032917141914, 0.12701380252838135, -0.062044110149145126, 0.2513098120689392] \n",
      "\n",
      "Sentence Embedding #7:  [0.3527010381221771, 0.2081831991672516, 0.196530282497406, 0.01412939466536045, 0.4114040434360504, -0.11001740396022797, -0.7598518133163452, -0.23773261904716492, 0.21618972718715668, 0.04337448254227638, -0.009442123584449291, 0.12009995430707932, 0.19873353838920593, -0.2239973098039627, 0.2292730212211609, 0.16251251101493835, -0.3181024193763733, -0.0012830018531531096, -0.013101363554596901, -0.6070674061775208, 0.5082429051399231, -0.11066173762083054, -0.10543221235275269, 0.06870473176240921, -0.03560367971658707, -1.0910638570785522, -0.3817167282104492, 0.06267320364713669, 0.2765468955039978, -0.19624170660972595, 2.7380640506744385, 0.017234284430742264, 0.04990572854876518, -0.27236661314964294, 0.1264142394065857, -0.03530830889940262, 0.05308012664318085, 0.12018389999866486, -0.011113389395177364, -0.18697331845760345, 0.3190408945083618, -0.006428159307688475, -0.012993931770324707, 0.22762437164783478, -0.07230773568153381, 0.152089923620224, -0.20201432704925537, 0.20428138971328735, 0.01430835947394371, 0.13926735520362854]\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence #5: \", docs[5], \"\\n\\nSentence #7: \", docs[7])\n",
    "print(\"\\nSentence Embedding #5: \", x[5], \"\\n\\nSentence Embedding #7: \", x[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_similarity =  0.8968743967161681\n"
     ]
    }
   ],
   "source": [
    "print(\"cosine_similarity = \", cosine_similarity(np.array(x[5]), np.array(x[7])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A value of 0.89 indicates that the sentences are close to each other, and so it is.\n",
    "\n",
    "Let's check on two positive sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence #1:  (['diehard', 'graco', 'daughter', 'born', 'graco', 'snugride', 'rated', 'infant', 'seat', 'stock', 'consumer', 'report', 'infant', 'seat', 'great', 'part', 'travel', 'system', 'loved', 'stroller', 'seat', 'outgrew', 'seat', 'around', 'year', 'tried', 'eddie', 'bauer', 'convertible', 'seat', 'next', 'horrible', 'strap', 'always', 'twisted', 'daughter', 'head', 'always', 'fell', 'forward', 'onto', 'chest', 'fell', 'asleep', 'quickly', 'changed', 'graco', 'comfortsport', 'overhead', 'drop', 'loved', 'padding', 'head', 'rested', 'side', 'carseat', 'instead', 'chest', 'strap', 'never', 'twisted', 'easy', 'heavy', 'around', 'airport', 'outgrew', 'seat', 'decided', 'make', 'switch', 'britax', 'although', 'roundabout', 'rate', 'went', 'marathon', 'thing', 'like', 'infant', 'latch', 'need', 'child', '3yrs', 'higher', 'weight', 'limit', 'roundabout', 'honda', 'accord', 'door', 'husband', 'drive', 'door', 'toyota', 'echo', 'tiny', 'seat', 'perfectly', 'well', 'vehicle', 'forward', 'facing', 'position', 'toyota', 'latch', 'honda', 'move', 'partly', 'fact', 'husband', 'excellent', 'installing', 'partly', 'seat', 'weight', 'quite', 'heavy', 'pain', 'haul', 'around', 'airport', 'husband', 'usually', 'loosens', 'strap', 'wear', 'like', 'backpack', 'airport', 'daughter', 'love', 'girly', 'print', 'granite', 'plenty', 'room', 'never', 'uncomfortable', 'sleeping', 'head', 'chest', 'either', 'though', 'bright', 'year', 'never', 'able', 'seat', 'love', 'fact', '65lbs', 'five', 'point', 'restraint', 'strap', 'never', 'twist', 'britax', 'also', 'thoughtful', 'thing', 'like', 'adding', 'velcro', 'side', 'seat', 'hold', 'strap', 'place', 'putting', 'child', 'padded', 'area', 'clip', 'child', 'burned', 'plastic', 'summer', 'freezing', 'winter', 'plan', 'staying', 'britax'], 'pos') \n",
      "\n",
      "Sentence #4:  (['best', 'carseat', 'perego', 'britax', 'weighs', 'pound', 'break', 'easy', 'adjust', 'best', 'feature', 'base', 'designed', 'move', 'graco', 'infant', 'seat', 'completely', 'flipped', 'sideways', 'made', 'turn', 'secure', 'even', 'special', 'belt', 'clip', 'side', 'canopy', 'floppy', 'still', 'star'], 'pos')\n",
      "\n",
      "Sentence Embedding #1:  [0.12711267170236737, 0.09236475469575064, 0.1266332291272759, -0.3013924247956748, 0.294683316376447, 0.19624675583867915, -0.3043137320414023, -0.03493387958727003, -0.0061060525680500305, -0.11686981351259852, 0.0443252307245803, -0.011276101143878014, -0.26993070709101774, 0.03044914987060379, 0.24125360968253, 0.33569200120642373, -0.15215016011392973, 0.10218650844810745, -0.10612362727034287, -0.5607575885239032, 0.018712027263038798, 0.197580213804826, -0.03804364950329961, 0.02118312717094773, 0.11329135551025755, -1.125217992181335, -0.2051172027914006, 0.36529712330122466, 0.3430080961860981, -0.17787827458741562, 2.094838625420638, 0.2979897524642407, -0.07502967412100056, -0.025080831284708564, 0.1635532991838504, 0.06514588520732388, 0.12988997355887227, 0.18282932833936372, 0.03143368033452979, -0.2425769464789681, -0.1511472329278502, 0.12855439917235303, -0.020095951993863194, 0.09676632000568522, -0.0409723698006322, -0.05890869530416577, 0.03822630964523491, -0.3853921659247532, 0.03171381270741952, -0.07378304081607472] \n",
      "\n",
      "Sentence Embedding #4:  [-0.024003845115657896, 0.032413323933724314, 0.27177543845027685, -0.19323246960993856, 0.24157759512308985, 0.07526156672975048, -0.2760535052511841, -0.2720376163961191, 0.08585320168640465, 0.045967813464812934, 0.02758947404799983, 0.036348608118714765, -0.12089703255333006, 0.2962150317616761, 0.10597237956244498, 0.26013640710152686, -0.014499877695925534, -0.014260866533732042, -0.17533631611149758, -0.5312268664129078, -0.12386704773234669, -0.007135688385460526, 0.0460448763333261, 0.014909687997715082, 0.0422693156870082, -0.8960106205195189, -0.11439847142901272, 0.3665759698487818, 0.38871053216280416, -0.17619909485802054, 2.1205024998635054, 0.12035856361035258, -0.14507543152649305, 0.09130546535016038, 0.19292692920134868, 0.1704116902546957, 0.11577803136606235, 0.2672465007635765, -0.20746869717549998, -0.485804874508176, 0.03960801559878746, 0.1643327432248043, 0.007201527449069545, 0.038974627503193915, -0.1504140937468037, -0.030792123987339437, 0.24923472222872078, -0.3328784227996948, -0.03202455530845327, 0.00434977759141475]\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence #1: \", docs[1], \"\\n\\nSentence #4: \", docs[4])\n",
    "print(\"\\nSentence Embedding #1: \", x[1], \"\\n\\nSentence Embedding #4: \", x[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_similarity =  0.9481159093219256\n"
     ]
    }
   ],
   "source": [
    "print(\"cosine_similarity = \", cosine_similarity(np.array(x[1]), np.array(x[4])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These sentences are also close to each other. \n",
    "\n",
    "So now let's check sentences with different states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence #1:  (['love', 'crate', 'smart', 'little', 'learned', 'zipper', 'undone', 'escape', 'course', 'home', 'find', 'small', 'gift', 'left', 'behind'], 'pos') \n",
      "\n",
      "Sentence #5:  (['product', 'generate', 'enough', 'heat', 'provide', 'relief', 'sore', 'aching', 'muscle', 'massaging', 'action', 'actually', 'vibration', 'action', 'heat', 'switched', 'vibration', 'action', 'reduced', 'dramatically'], 'neg')\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence #1: \", docs[0], \"\\n\\nSentence #5: \", docs[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_similarity =  0.7410293614966914\n"
     ]
    }
   ],
   "source": [
    "print(\"cosine_similarity = \", cosine_similarity(np.array(x[0]), np.array(x[6])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, our average embedding still has some problems with separating different classes with cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
